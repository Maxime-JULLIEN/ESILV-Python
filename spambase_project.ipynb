{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Python For Data Analysis : Final Project\n",
    "## spambase dataset : Adam DBZ, Maxime JULLIEN, Othman SEQQAT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('spambase/spambase.names','r') as f:\n",
    "    features = []\n",
    "    for line in f.readlines()[33:]:\n",
    "        features.append(line.split(':')[0])\n",
    "    features.append('spam')\n",
    "    f.close()\n",
    "\n",
    "spambase_df = pd.read_csv('spambase/spambase.data', names=features)"
   ]
  },
  {
   "source": [
    "* #### lets take a look on our Data Frame"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "\n",
       "[3 rows x 58 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_freq_make</th>\n      <th>word_freq_address</th>\n      <th>word_freq_all</th>\n      <th>word_freq_3d</th>\n      <th>word_freq_our</th>\n      <th>word_freq_over</th>\n      <th>word_freq_remove</th>\n      <th>word_freq_internet</th>\n      <th>word_freq_order</th>\n      <th>word_freq_mail</th>\n      <th>...</th>\n      <th>char_freq_;</th>\n      <th>char_freq_(</th>\n      <th>char_freq_[</th>\n      <th>char_freq_!</th>\n      <th>char_freq_$</th>\n      <th>char_freq_#</th>\n      <th>capital_run_length_average</th>\n      <th>capital_run_length_longest</th>\n      <th>capital_run_length_total</th>\n      <th>spam</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00</td>\n      <td>0.64</td>\n      <td>0.64</td>\n      <td>0.0</td>\n      <td>0.32</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.778</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>3.756</td>\n      <td>61</td>\n      <td>278</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.21</td>\n      <td>0.28</td>\n      <td>0.50</td>\n      <td>0.0</td>\n      <td>0.14</td>\n      <td>0.28</td>\n      <td>0.21</td>\n      <td>0.07</td>\n      <td>0.00</td>\n      <td>0.94</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.132</td>\n      <td>0.0</td>\n      <td>0.372</td>\n      <td>0.180</td>\n      <td>0.048</td>\n      <td>5.114</td>\n      <td>101</td>\n      <td>1028</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.06</td>\n      <td>0.00</td>\n      <td>0.71</td>\n      <td>0.0</td>\n      <td>1.23</td>\n      <td>0.19</td>\n      <td>0.19</td>\n      <td>0.12</td>\n      <td>0.64</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>0.01</td>\n      <td>0.143</td>\n      <td>0.0</td>\n      <td>0.276</td>\n      <td>0.184</td>\n      <td>0.010</td>\n      <td>9.821</td>\n      <td>485</td>\n      <td>2259</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows Ã— 58 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "spambase_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of the original spambase Data Frame : (4601, 58)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of the original spambase Data Frame : {spambase_df.shape}\")"
   ]
  },
  {
   "source": [
    "* lets see the columns type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "word_freq_make                float64\n",
       "word_freq_address             float64\n",
       "word_freq_all                 float64\n",
       "word_freq_3d                  float64\n",
       "word_freq_our                 float64\n",
       "word_freq_over                float64\n",
       "word_freq_remove              float64\n",
       "word_freq_internet            float64\n",
       "word_freq_order               float64\n",
       "word_freq_mail                float64\n",
       "word_freq_receive             float64\n",
       "word_freq_will                float64\n",
       "word_freq_people              float64\n",
       "word_freq_report              float64\n",
       "word_freq_addresses           float64\n",
       "word_freq_free                float64\n",
       "word_freq_business            float64\n",
       "word_freq_email               float64\n",
       "word_freq_you                 float64\n",
       "word_freq_credit              float64\n",
       "word_freq_your                float64\n",
       "word_freq_font                float64\n",
       "word_freq_000                 float64\n",
       "word_freq_money               float64\n",
       "word_freq_hp                  float64\n",
       "word_freq_hpl                 float64\n",
       "word_freq_george              float64\n",
       "word_freq_650                 float64\n",
       "word_freq_lab                 float64\n",
       "word_freq_labs                float64\n",
       "word_freq_telnet              float64\n",
       "word_freq_857                 float64\n",
       "word_freq_data                float64\n",
       "word_freq_415                 float64\n",
       "word_freq_85                  float64\n",
       "word_freq_technology          float64\n",
       "word_freq_1999                float64\n",
       "word_freq_parts               float64\n",
       "word_freq_pm                  float64\n",
       "word_freq_direct              float64\n",
       "word_freq_cs                  float64\n",
       "word_freq_meeting             float64\n",
       "word_freq_original            float64\n",
       "word_freq_project             float64\n",
       "word_freq_re                  float64\n",
       "word_freq_edu                 float64\n",
       "word_freq_table               float64\n",
       "word_freq_conference          float64\n",
       "char_freq_;                   float64\n",
       "char_freq_(                   float64\n",
       "char_freq_[                   float64\n",
       "char_freq_!                   float64\n",
       "char_freq_$                   float64\n",
       "char_freq_#                   float64\n",
       "capital_run_length_average    float64\n",
       "capital_run_length_longest      int64\n",
       "capital_run_length_total        int64\n",
       "spam                            int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "spambase_df.dtypes"
   ]
  },
  {
   "source": [
    "* Lets verify if there is any null values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "spambase_df.isnull().values.any()"
   ]
  },
  {
   "source": [
    "* Lets have some informations on our features  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000  ...  4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413  ...     0.038575     0.139030   \n",
       "std           0.278616        0.644755  ...     0.243471     0.270355   \n",
       "min           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "25%           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "50%           0.000000        0.000000  ...     0.000000     0.065000   \n",
       "75%           0.000000        0.160000  ...     0.000000     0.188000   \n",
       "max           5.260000       18.180000  ...     4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total         spam  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_freq_make</th>\n      <th>word_freq_address</th>\n      <th>word_freq_all</th>\n      <th>word_freq_3d</th>\n      <th>word_freq_our</th>\n      <th>word_freq_over</th>\n      <th>word_freq_remove</th>\n      <th>word_freq_internet</th>\n      <th>word_freq_order</th>\n      <th>word_freq_mail</th>\n      <th>...</th>\n      <th>char_freq_;</th>\n      <th>char_freq_(</th>\n      <th>char_freq_[</th>\n      <th>char_freq_!</th>\n      <th>char_freq_$</th>\n      <th>char_freq_#</th>\n      <th>capital_run_length_average</th>\n      <th>capital_run_length_longest</th>\n      <th>capital_run_length_total</th>\n      <th>spam</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>...</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n      <td>4601.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.104553</td>\n      <td>0.213015</td>\n      <td>0.280656</td>\n      <td>0.065425</td>\n      <td>0.312223</td>\n      <td>0.095901</td>\n      <td>0.114208</td>\n      <td>0.105295</td>\n      <td>0.090067</td>\n      <td>0.239413</td>\n      <td>...</td>\n      <td>0.038575</td>\n      <td>0.139030</td>\n      <td>0.016976</td>\n      <td>0.269071</td>\n      <td>0.075811</td>\n      <td>0.044238</td>\n      <td>5.191515</td>\n      <td>52.172789</td>\n      <td>283.289285</td>\n      <td>0.394045</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.305358</td>\n      <td>1.290575</td>\n      <td>0.504143</td>\n      <td>1.395151</td>\n      <td>0.672513</td>\n      <td>0.273824</td>\n      <td>0.391441</td>\n      <td>0.401071</td>\n      <td>0.278616</td>\n      <td>0.644755</td>\n      <td>...</td>\n      <td>0.243471</td>\n      <td>0.270355</td>\n      <td>0.109394</td>\n      <td>0.815672</td>\n      <td>0.245882</td>\n      <td>0.429342</td>\n      <td>31.729449</td>\n      <td>194.891310</td>\n      <td>606.347851</td>\n      <td>0.488698</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.588000</td>\n      <td>6.000000</td>\n      <td>35.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.065000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.276000</td>\n      <td>15.000000</td>\n      <td>95.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.420000</td>\n      <td>0.000000</td>\n      <td>0.380000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.160000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.188000</td>\n      <td>0.000000</td>\n      <td>0.315000</td>\n      <td>0.052000</td>\n      <td>0.000000</td>\n      <td>3.706000</td>\n      <td>43.000000</td>\n      <td>266.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.540000</td>\n      <td>14.280000</td>\n      <td>5.100000</td>\n      <td>42.810000</td>\n      <td>10.000000</td>\n      <td>5.880000</td>\n      <td>7.270000</td>\n      <td>11.110000</td>\n      <td>5.260000</td>\n      <td>18.180000</td>\n      <td>...</td>\n      <td>4.385000</td>\n      <td>9.752000</td>\n      <td>4.081000</td>\n      <td>32.478000</td>\n      <td>6.003000</td>\n      <td>19.829000</td>\n      <td>1102.500000</td>\n      <td>9989.000000</td>\n      <td>15841.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 58 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "spambase_df.describe()"
   ]
  },
  {
   "source": [
    "## Setting up a train dataset and a test dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of the train data values : (3450, 57)\nShape of the test data values : (1151, 57)\n\nShape of the train label values : (3450,)\nShape of the test label values :  (1151,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = spambase_df.iloc[:,:-1]     # Removing the spam column from the original dataset\n",
    "Y = spambase_df.spam            # choosing only the spam column from the dataset\n",
    "\n",
    "# Splitting : we're train with 75% and keep 25% for testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "print(f\"Shape of the train data values : {X_train.shape}\")\n",
    "print(f\"Shape of the test data values : {X_test.shape}\\n\")\n",
    "\n",
    "print(f\"Shape of the train label values : {Y_train.shape}\")\n",
    "print(f\"Shape of the test label values :  {Y_test.shape}\")"
   ]
  },
  {
   "source": [
    "## Machine Learning models:\n",
    "We are going to use these models:\n",
    "* ### Linear Discriminant Analysis\n",
    "* ### Logistic Regression\n",
    "* ### Gradient Boosting Classifier\n",
    "* ### Random Forest Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* Lets import the models from scikit-learn "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set the number of decimals you want in your scores\n",
    "p = 5"
   ]
  },
  {
   "source": [
    "* Lets fit the models\n",
    "### Linear Discriminant Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Linear Discriminant Analysis score on train dataset : 0.88667\nThe Linear Discriminant Analysis score on test dataset : 0.90009\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_components=1)\n",
    "\n",
    "lda.fit_transform(X_train, Y_train)\n",
    "lda.transform(X_test)\n",
    "\n",
    "print(f\"The Linear Discriminant Analysis score on train dataset : {round(lda.score(X_train, Y_train), p)}\")\n",
    "print(f\"The Linear Discriminant Analysis score on test dataset : {round(lda.score(X_test, Y_test), p)}\")\n"
   ]
  },
  {
   "source": [
    "### Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Logistic Regression score on train dataset : 0.93188\nThe Logistic Regression score on train dataset : 0.93397\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(max_iter= 3000, random_state=10)\n",
    "logReg.fit(X_train, Y_train)\n",
    "\n",
    "print(f\"The Logistic Regression score on train dataset : {round(logReg.score(X_train, Y_train), p)}\")\n",
    "print(f\"The Logistic Regression score on test dataset : {round(logReg.score(X_test, Y_test), p)}\")"
   ]
  },
  {
   "source": [
    "### Gradient Boosting Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Gradient Boosting Classifier on train dataset : 0.96406\nThe Gradient Boosting Classifier on test dataset : 0.94266\n"
     ]
    }
   ],
   "source": [
    "gradBoost = GradientBoostingClassifier(random_state=10)\n",
    "gradBoost.fit(X_train, Y_train)\n",
    "\n",
    "print(f\"The Gradient Boosting Classifier on train dataset : {round(gradBoost.score(X_train, Y_train), p)}\")\n",
    "print(f\"The Gradient Boosting Classifier on test dataset : {round(gradBoost.score(X_test, Y_test), p)}\")"
   ]
  },
  {
   "source": [
    "### Random Forest Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Random Forest Classifier on train dataset : 0.99942\nThe Random Forest Classifier on test dataset : 0.95482\n"
     ]
    }
   ],
   "source": [
    "randForest = RandomForestClassifier(random_state=10)\n",
    "randForest.fit(X_train, Y_train)\n",
    "\n",
    "print(f\"The Random Forest Classifier on train dataset : {round(randForest.score(X_train, Y_train), p)}\")\n",
    "print(f\"The Random Forest Classifier on test dataset : {round(randForest.score(X_test, Y_test), p)}\")"
   ]
  },
  {
   "source": [
    "* #### Lets create a dictionnary for these models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"lda\": {\"name\" : \"Linear Discriminant Analysis\", \"model\": lda, \"test_score\": round(lda.score(X_test, Y_test), p)},\n",
    "\"logReg\": {\"name\" : \"Logistic Regression\",\"model\": logReg, \"test_score\": round(logReg.score(X_test, Y_test), p)},\n",
    "\"gradBoost\": {\"name\" : \"Gradient Boosting Classifier\",\"model\": gradBoost, \"test_score\": round(gradBoost.score(X_test, Y_test), p)},\n",
    "\"randForest\": {\"name\" : \"Random Forest Classifier\",\"model\": randForest, \"test_score\": round(randForest.score(X_test, Y_test), p)}}"
   ]
  },
  {
   "source": [
    "### Comparing these Machine Learning models on this dataset "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* #### We make a prediction for each model from the previous ones, then we find out it's precision"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the accuracy and the precision functions\n",
    "from sklearn.metrics import accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    Y_pred = eval(model).predict(X_test)\n",
    "    models[model].update({'accuracy': round(accuracy_score(Y_test, Y_pred), p)})\n",
    "    models[model].update({'precision': round(precision_score(Y_test, Y_pred), p)})"
   ]
  },
  {
   "source": [
    "* #### Lets print the scores for each model to make comparisons between them"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                          model  accuracy  precision\n",
       "3      Random Forest Classifier   0.95482    0.94518\n",
       "2  Gradient Boosting Classifier   0.94266    0.93946\n",
       "0  Linear Discriminant Analysis   0.90009    0.92768\n",
       "1           Logistic Regression   0.93397    0.92070"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>accuracy</th>\n      <th>precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>Random Forest Classifier</td>\n      <td>0.95482</td>\n      <td>0.94518</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Gradient Boosting Classifier</td>\n      <td>0.94266</td>\n      <td>0.93946</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Linear Discriminant Analysis</td>\n      <td>0.90009</td>\n      <td>0.92768</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Logistic Regression</td>\n      <td>0.93397</td>\n      <td>0.92070</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame(data=zip([models[model]['name'] for model in models], [models[model]['accuracy'] for model in models], [models[model]['precision'] for model in models]), columns=['model', 'accuracy', 'precision'])\n",
    "\n",
    "comparison_df.sort_values(by=['precision'], ascending=False)"
   ]
  },
  {
   "source": [
    "We can see the most precise model from what we did is the : Random Forest Classifier\n",
    "\n",
    "Lets do some tunning "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exporting our model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['randForest.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "filename = \"randForest.pkl\"\n",
    "joblib.dump(randForest, filename)"
   ]
  }
 ]
}